{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c1b991-ffc3-4a28-bb17-cf9d11d4a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Ahsan Aziz - Security Consultant - The Missing Link\n",
    "# Date: 20/04/2022\n",
    "# Version: 1.0\n",
    "# Inspired by Omar's BSides presentation: https://www.youtube.com/watch?v=LTNKMA65BtI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388e44bc-f5e0-4da8-999d-3838cb45fb93",
   "metadata": {},
   "source": [
    "# Description\n",
    "Automating OSINT (and a bit of scanning). If everything goes well, it will produce a zip file with all the information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73731d8-0d48-45cf-aa00-2c98a579da0f",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Prior to running any cells assign a value to the variables and run the cell. This will change the targets for enumeration without needing to modify the script parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e562ae-5482-4f75-b74b-559d25b9b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCAN_TYPE = \"A\" #P=Passive, no interaction with the org's assets (except DNS requests). N=Normal, non-malicious TCP/HTTP requests sent. A=Aggressive, a number of malicious HTTP requests sent for vulnerability identification.\n",
    "DOMAIN = \"tesla.com\"  #root domain (required)\n",
    "IP_RANGE = \"184.30.18.0/24\" #nmap format. If empty, the IPs of subdomains will be used\n",
    "ORG_NAME = \"tesla\"  #this will be used in linkedIn search and cloud enumeration (required)\n",
    "FOLDER_NAME = \"tesla\" #this folder will contain the discovered recon data (required)\n",
    "VHOST = \"tesla.com\" #domain for virtual host scanning, leave it empty if vhost scan is not required\n",
    "EMAIL_FORMAT = \"{f}{last}\" #e.g. {f}{last}, check hunter.io if you're not sure about the email format\n",
    "DEHASHED_USER = \"\" #dehashed username\n",
    "DEHASHED_KEY = \"\"  #dehashed API Key\n",
    "CHAOS_KEY = \"\" #CHAOS API key for subdomain enum, request your key: https://forms.gle/GP5nTamxJPfiMaBn9\n",
    "CENSYS_KEY = \"\" #Censys API key for subdomain enum, request your key: https://censys.io/register\n", 
    "CERTSPOTTER_KEY = \"\" #Certspotter API key for subdomain enum, request your key: https://sslmate.com/signup?for=certspotter_api\n",
    "SECURITYTRAILS_KEY = \"\" #SecurityTrails API key for subdomain enum, request your key: https://securitytrails.com/app/signup\n",
    "SHODAN_KEY = \"\" #Shodan API key for subdomain enum, request your key: https://account.shodan.io/login\n",
    "URLSCAN_KEY = \"\" #URLScan API key for subdomain enum, request your key: https://urlscan.io/user/signup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea24eb-fa4b-4ca0-9c40-8813e2e3342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Setting up project folders and files\")\n",
    "!mkdir $FOLDER_NAME\n",
    "!nmap -sL -n $IP_RANGE | grep 'Nmap scan report for' | cut -f 5 -d ' ' > $FOLDER_NAME/IP_Range.txt\n",
    "!mkdir $FOLDER_NAME/Screenshots\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f081cb1d-fc8d-4f96-bc1f-7dc2f2c0aaca",
   "metadata": {},
   "source": [
    "# NMAP\n",
    "All port scan is not included as it may take a while, add flags as per your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d330caa3-b91f-4d60-8126-55a454fbe087",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IP_RANGE != \"\" and SCAN_TYPE != \"P\":\n",
    "        !nmap -Pn $IP_RANGE >> $FOLDER_NAME/nmap.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d960dc26-5c1b-4465-9908-f0fea2a9ff3d",
   "metadata": {},
   "source": [
    "# Subdomain Enumeration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d41c6-3a5a-4a01-9aec-031a1cfa29ea",
   "metadata": {},
   "source": [
    "## Subfinder\n",
    "subfinder: https://github.com/projectdiscovery/subfinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17481ddb-df4d-4517-90be-e05687073d34",
   "metadata": {},
   "outputs": [],
   "source": [
   "#update config file with provider config\n",
   "!echo \"provider-config: ~/.config/subfinder/provider-config.yaml\" >> ~/.config/subfinder/config.yaml\n",
   "!echo \"\" > ~/.config/subfinder/provider-config.yaml\n\n",
   "#update the API keys in provider config file\n",
    "if CENSYS_KEY != \"\":\n",
    "    !echo \"censys:\" >> ~/.config/subfinder/provider-config.yaml\n",
    "    !echo \" - $CENSYS_KEY\" >> ~/.config/subfinder/provider-config.yaml\n",
    "if CERTSPOTTER_KEY != \"\":\n",
    "    !echo \"certspotter:\" >> ~/.config/subfinder/provider-config.yaml\n",
    "    !echo \" - $CERTSPOTTER_KEY\" >> ~/.config/subfinder/provider-config.yaml\n",
    "if SECURITYTRAILS_KEY != \"\":\n",
    "    !echo \"securitytrails:\" >> ~/.config/subfinder/provider-config.yaml\n",
    "    !echo \" - $SECURITYTRAILS_KEY\" >> ~/.config/subfinder/provider-config.yaml\n",
    "if SHODAN_KEY != \"\":\n",
    "    !echo \"shodan:\" >> ~/.config/subfinder/provider-config.yaml\n",
    "    !echo \" - $SHODAN_KEY\" >> ~/.config/subfinder/provider-config.yaml\n",
    "if URLSCAN_KEY != \"\":\n",
    "    !echo \"urlscan:\" >> ~/.config/subfinder/provider-config.yaml\n",
    "    !echo \" - $URLSCAN_KEY\" >> ~/.config/subfinder/provider-config.yaml\n\n",
    "print(\"Scraping domains using subfinder...\")\n",
    "!subfinder -silent -d $DOMAIN -o subfinder.csv \n",
    "!cat subfinder.csv |anew $FOLDER_NAME/Subdomains.csv\n",
    "#anew: https://github.com/tomnomnom/anew\n",
    "!rm subfinder.csv\n",
    "print(\"Done. The file ./{}/subdomains.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ec795-8096-4536-8131-afa25bca66e7",
   "metadata": {},
   "source": [
    "## Amass\n",
    "\n",
    "OWASP's Amass: https://github.com/OWASP/Amass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc971a8b-38f5-4a42-b978-9375e9f9abfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCAN_TYPE != \"P\":\n",
    "    print(\"Scraping domains using OWASP's amass...\")\n",
    "    !amass enum -d $DOMAIN -o amass.csv -r 8.8.8.8\n",
    "    !cat amass.csv |anew $FOLDER_NAME/Subdomains.csv\n",
    "    !rm amass.csv\n",
    "    print(\"Done. The file ./{}/subdomains.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c69ac-8eed-4c56-b814-7eaff016552f",
   "metadata": {},
   "source": [
    "## DNS database search - CHAOS and RAPID7 SONAR\n",
    "Chaos: https://chaos.projectdiscovery.io/#/docs  \n",
    "Rapid7 Sonar: https://sonar.omnisint.io/\n",
    "\n",
    "crobat is currently commented out, it's not always available, run it if you'd like to enumerate domains from Rapid7's database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55698fb-bad7-4bcc-8638-8021cbb7f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Querying Rapid7 Sonar using crobat...\")\n",
    "#!crobat -s $DOMAIN >> crobat.csv\n",
    "#!cat crobat.csv |anew $FOLDER_NAME/Subdomains.csv    \n",
    "#!rm crobat.csv\n",
    "#print(\"Done. The file ./{}/Subdomains.csv is updated!\".format(FOLDER_NAME))\n",
    "\n",
    "if CHAOS_KEY != \"\":\n",
    "    print(\"Querying CHAOS...\")\n",
    "    !chaos -key $CHAOS_KEY -d $DOMAIN -silent -o chaos.csv\n",
    "    !cat chaos.csv |dnsx -silent|anew $FOLDER_NAME/Subdomains.csv\n",
    "    !rm chaos.csv\n",
    "    print(\"Done. The file ./{}/Subdomains.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70af7b3b-4fef-41bf-9281-7c90a08eee50",
   "metadata": {},
   "source": [
    "## Reverse DNS\n",
    "Performing a reverse DNS on provided IP_RANGE using Rapid7 Sonar: https://sonar.omnisint.io/\n",
    "\n",
    "##### Tip: You may want to pick a domain (other than root domain) from the output of this section and repeat the subdomain enumeration steps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af09543-7834-4dc8-bf70-54bf2159d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IP_RANGE != \"\":\n",
    "    print(\"Reverse querying Rapid7 Sonar using crobat...\")\n",
    "    !cp $FOLDER_NAME/IP_Range.txt ip.txt\n",
    "    !crobat -r ip.txt >> crobat_reverse.csv\n",
    "    !cat crobat_reverse.csv |anew $FOLDER_NAME/Subdomains.csv\n",
    "    !rm crobat_reverse.csv\n",
    "    !rm ip.txt\n",
    "    print(\"Done. The file ./{}/Subdomains.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a868e-5853-4aaa-9beb-40d6a7513d60",
   "metadata": {},
   "source": [
    "## Subdomain Bruteforcing\n",
    "shuffledns: https://github.com/projectdiscovery/shuffledns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aac7e7-018a-43c8-b017-b8aa8d6c8f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCAN_TYPE == \"A\":\n",
    "    print(\"Bruteforcing using shuffledns with commonspeak wordlist... \")\n",
    "    !shuffledns -silent -d $DOMAIN -w ../commonspeak2.txt -r ../resolvers.txt >> shuffledns.csv\n",
    "    !cat shuffledns.csv |anew $FOLDER_NAME/Subdomains.csv\n",
    "    !rm shuffledns.csv\n",
    "    print(\"Done. The file ./{}/Subdomains.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2258d83-2adc-4965-ae2a-4b071caa7d9e",
   "metadata": {},
   "source": [
    "## In-scope Domains\n",
    "DNS probing subdomains.csv and filtering subdomains as per IP_RANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb62ab-6223-4b71-96a9-00954a69a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IP_RANGE != \"\":\n",
    "    print(\"First let's sort and get unique domains...\")\n",
    "    !sort -u $FOLDER_NAME/Subdomains.csv > uniq.csv\n",
    "    !cp uniq.csv $FOLDER_NAME/Subdomains.csv\n",
    "    !rm uniq.csv\n",
    "    print(\"Done. The file ./{}/Subdomains.csv is updated!\".format(FOLDER_NAME))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f6a8b7-53f6-47cf-8f29-87ff03d69409",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IP_RANGE != \"\":\n",
    "    print(\"Now let's resolve the domains and compare it with the provided IP_RANGE...\")\n",
    "    !cp $FOLDER_NAME/Subdomains.csv subs.txt\n",
    "    !cp $FOLDER_NAME/IP_Range.txt ip.txt\n",
    "    #execute the comparison three times to make sure no subdomains is missed\n",
    "    for i in range(3):\n",
    "        !cat subs.txt |dnsx| while read line; do if grep -q -w `echo \"$line\" | dnsx -silent -a -resp |cut -d \" \" -f 2|cut -d \"]\" -f 1 |cut -d \"[\" -f 2` ip.txt;then echo $line >> tmp.csv;fi; done\n",
    "    !rm ip.txt\n",
    "    !rm subs.txt\n",
    "    !cat tmp.csv |anew $FOLDER_NAME/In_Scope_Subdomains.csv\n",
    "    print(\"Done. The file ./{}/In_Scope_Subdomains.csv is updated!\".format(FOLDER_NAME))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325518b0-aa73-426a-84ae-862db0f5a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IP_RANGE != \"\":\n",
    "    print(\"Creating the csv with two columns (domain and the corresponding IP address)...\")\n",
    "    !cat tmp.csv | while read domain; do echo \"$domain,`echo $domain | dnsx -silent -a -resp |cut -d \" \" -f 2|cut -d \"]\" -f 1 |cut -d \"[\" -f 2`\" >> output.csv; done\n",
    "    !cat output.csv |anew $FOLDER_NAME/In_Scope_Subdomains_IPs.csv\n",
    "    !rm output.csv                                                                                                                                    \n",
    "    !rm tmp.csv\n",
    "    print(\"Done. The file ./{}/In_Scope_Subdomains_IPs.csv is updated!\".format(FOLDER_NAME)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cefc17-3232-4c3c-ad91-0404d14a426b",
   "metadata": {},
   "source": [
    "## Resolving domains\n",
    "\n",
    "Checking if all subdomains resolve to IP addresses using dnsx, the domains which do not resolve may be vulnerable to takover.\n",
    "\n",
    "dnsx: https://github.com/projectdiscovery/dnsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d5b4fa-2952-4c49-9112-ad7a737e2794",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resolving domains using dnsx ... \")\n",
    "!cat $FOLDER_NAME/Subdomains.csv|dnsx -silent|anew tmp.csv\n",
    "print(\"Creating the csv with two columns (domain and the corresponding IP address)...\")\n",
    "!cat tmp.csv | while read domain; do echo \"$domain,`echo $domain | dnsx -silent -a -resp |cut -d \" \" -f 2|cut -d \"]\" -f 1 |cut -d \"[\" -f 2`\" >> output.csv; done\n",
    "!cat output.csv |anew $FOLDER_NAME/Subdomains_Resolved.csv\n",
    "print(\"Done. The file ./{}/Subdomains_Resolved.csv is updated!\".format(FOLDER_NAME))                                                                                                                                     \n",
    "!rm output.csv \n",
    "                                                                                                                                    \n",
    "if IP_RANGE == \"\": \n",
    "    print(\"Creating Subdomains_IPs.csv file for saving IPs of discovered subdomains... \")                                                                                                                                   \n",
    "    !dnsx -silent -a -resp -l tmp.csv|cut -d \" \" -f 2|cut -d \"]\" -f 1 |cut -d \"[\" -f 2 >> output.csv\n",
    "    !cat output.csv |anew $FOLDER_NAME/Subdomain_IPs.csv\n",
    "    !rm output.csv                                      \n",
    "    print(\"Done. The file ./{}/Subdomains_IPs.csv is updated!\".format(FOLDER_NAME))                                                                                                                                \n",
    "!rm tmp.csv                                                                                                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6336dc7d-e078-496c-8ffb-b2139efaf4c7",
   "metadata": {},
   "source": [
    "## Shodoan's Nrich\n",
    "\n",
    "A command-line tool to quickly analyze all IPs in a file and see which ones have open ports/ vulnerabilities. \n",
    "\n",
    "nrich: https://gitlab.com/shodan-public/nrich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac07c0-586d-4083-9645-a829d6ae484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IP_RANGE != \"\":\n",
    "    print(\"Querying shodan's database for IP_RANGE provided...\")\n",
    "    !cp $FOLDER_NAME/IP_Range.txt ip.txt\n",
    "else:\n",
    "    print(\"Querying shodan's database for Subdomains_IPs...\")\n",
    "    !cp $FOLDER_NAME/Subdomain_IPs.csv ip.txt\n",
    "\n",
    "!cat ip.txt |nrich - >> $FOLDER_NAME/Shodan_Nrich.txt \n",
    "!rm ip.txt    \n",
    "print(\"Done. The file ./{}/Shodan_Nrich.txt is updated!\".format(FOLDER_NAME)) \n",
    "print(\"Note: Shodan_Nrich.txt will not be part of final excel file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d45c2e-f7b3-4150-989d-dcae082c7668",
   "metadata": {},
   "source": [
    "# Subdomain Takeover\n",
    "\n",
    "Subzy: https://github.com/LukaSikic/subzy\n",
    "\n",
    "##### Note: Since the cloud service providers keep updating their subdomain policies, there is no reliable automated tool to confirm subdomain takeover. Subzy may provide false positives. To get the latest information about subdomain takeover, head over to: https://github.com/EdOverflow/can-i-take-over-xyz/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ba5ca-17a7-42b5-9228-daa866cc2107",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running subzy on all subdomains (and not just the domains in scope)...\")\n",
    "!cp $FOLDER_NAME/Subdomains.csv subs.csv\n",
    "!subzy --hide_fails -targets subs.csv >> Subdomain_Takeover.txt \n",
    "!rm subs.csv\n",
    "!cat Subdomain_Takeover.txt |anew $FOLDER_NAME/Subdomain_Takeover.txt\n",
    "!rm Subdomain_Takeover.txt\n",
    "print(\"Done. The file ./{}/Subdomain_Takeover.txt is updated!\".format(FOLDER_NAME))\n",
    "print(\"Note: Subdomain_Takeover.txt will not be part of final excel file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e30296-8568-4352-b9f9-320a5eee1b57",
   "metadata": {},
   "source": [
    "# HTTP Probing\n",
    "Probing in-scope domains using httpx: https://github.com/projectdiscovery/httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410cbf8c-cfd5-46c3-8eac-f25c8e197d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCAN_TYPE != \"P\":\n",
    "    print(\"Probing using httpx...\")\n",
    "    if IP_RANGE == \"\":   \n",
    "        !cp $FOLDER_NAME/Subdomains.csv subs.csv\n",
    "        !cat subs.csv |dnsx -silent |httpx -p -silent >> httpx.csv\n",
    "        !cat httpx.csv |anew $FOLDER_NAME/Probed_Subdomains.csv\n",
    "        print(\"Done. The file ./{}/Probed_Subdomains.csv is updated!\".format(FOLDER_NAME))\n",
    "    else:\n",
    "        !cp $FOLDER_NAME/In_Scope_Subdomains.csv subs.csv\n",
    "        !cat subs.csv |dnsx -silent|httpx -silent >> httpx.csv\n",
    "        !cat httpx.csv |anew $FOLDER_NAME/Probed_In_Scope_Subdomains.csv\n",
    "        print(\"Done. The file ./{}/Probed_In_Scope_Subdomains.csv is updated!\".format(FOLDER_NAME))\n",
    "    !rm subs.csv\n",
    "    !rm httpx.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df4c86-c7a9-45b4-ae4b-57c380d4f460",
   "metadata": {},
   "source": [
    "# Screenshoting\n",
    "\n",
    "Eyewitness: https://github.com/FortyNorthSecurity/EyeWitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb7afb-d91f-44d6-b74f-8e6bf7a94b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$IP_RANGE\" \"$FOLDER_NAME\" \"$SCAN_TYPE\"\n",
    "if [ \"$3\" != \"P\" ]; then\n",
    "echo \"Screenthoting probed domians using EyeWitness...\"\n",
    "if [ \"$1\" == \"\" ]; then\n",
    "    cp $2/Probed_Subdomains.csv subs.csv\n",
    "    python3 ../EyeWitness/Python/EyeWitness.py --delay 2 --no-prompt -f subs.csv -d $2/Screenshots/\n",
    "    echo \"Done. The screenshots for Probed_Subdomains are saved in ./$2/screenshots/report.html\"\n",
    "else \n",
    "    cp $2/Probed_In_Scope_Subdomains.csv subs.csv\n",
    "    python3 ../EyeWitness/Python/EyeWitness.py --delay 2 --no-prompt -f subs.csv -d $2/Screenshots/\n",
    "    echo \"Done. The screenshots for Probed_In_Scope_Subdomains are saved in ./{}/screenshots/report.html\"\n",
    "fi\n",
    "rm subs.csv\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e48e6c-c11e-4f0b-9f33-585a7d0dee15",
   "metadata": {},
   "source": [
    "# Nuclei Scan\n",
    "Nuclei is a web scanner, it can detect technologies in use, identify CORS and TLS issues, and can scan for famous zero days such as log4j. Give it a go, it's pretty good!\n",
    "\n",
    "**This might take sometime depending on the number of domains. It is a bit noisey, may send hundreds of GET/POST requests, be careful in a red team engagement.**\n",
    "\n",
    "Nuclei: https://github.com/projectdiscovery/nuclei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abcc504-40e7-4b37-a437-748f2dac9c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCAN_TYPE == \"A\":\n",
    "    print(\"Scanning probed domains using nuclei...\")\n",
    "    !nuclei -update\n",
    "    !nuclei -update-templates #let's first update nuclei database\n",
    "    if IP_RANGE == \"\":\n",
    "        !cp $FOLDER_NAME/Probed_Subdomains.csv subs.csv\n",
    "        !nuclei -l subs.csv |anew $FOLDER_NAME/Nuclei.txt\n",
    "        print(\"Done. Nuclei resuts for Probed_Subdomains saved in  ./{}/Nuclei.txt\".format(FOLDER_NAME))\n",
    "    else:\n",
    "        !cp $FOLDER_NAME/Probed_In_Scope_Subdomains.csv subs.csv\n",
    "        !nuclei -l subs.csv |anew $FOLDER_NAME/Nuclei.txt\n",
    "        print(\"Done. Nuclei resuts for Probed_In_Scope_Subdomains saved in  ./{}/Nuclei.txt\".format(FOLDER_NAME))\n",
    "    !rm subs.csv        \n",
    "    print(\"Note: Nuclei.txt will not be part of final excel file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6238794d-73e6-4938-bd7c-d41e9b986bfc",
   "metadata": {},
   "source": [
    "# VHOST Scanning \n",
    "If a reverse proxy like nginx is in use, some subdomains may not have DNS entries; using a fuzzer (ffuf in this case) we can try bruteforcing the virtual hosts with the host header. It may produce false positives as some web servers respond to all hosts.\n",
    "\n",
    "FFUF: https://github.com/ffuf/ffuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722834d0-6d2b-4542-b213-a974523dbde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCAN_TYPE == \"A\":\n",
    "    if VHOST != \"\":\n",
    "        print(\"Bruteforcing vhosts using commonspeak wordlist. ... \")\n",
    "        !ffuf -s -u \"https://$VHOST\" -w ../commonspeak2.txt -H \"Host: FUZZ.$DOMAIN\" -of csv -o ffuf.csv\n",
    "        !cat ffuf.csv |sort -u |anew $FOLDER_NAME/Vhosts.csv\n",
    "        !rm ffuf.csv\n",
    "        print(\"Done. The file ./{}/Vhosts.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadaa9fe-36ea-402c-822e-ec220ba95982",
   "metadata": {},
   "source": [
    "# Cloud Storage Misconfigurations\n",
    "Cloudenum: https://github.com/initstring/cloud_enum\n",
    "\n",
    "If no bucket is found, try with different keywords.\n",
    "\n",
    "### AWS cli commands:\n",
    "```\n",
    "aws s3 ls s3://{bucket} --no-sign-request\n",
    "aws s3 cp abc.txt s3://{bucket}/abc.txt --no-sign-request\n",
    "aws s3 rm s3://{bucket}/abc.txt --no-sign-request\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd9e814-d47f-47a8-8a3a-7ebf2d5606a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Enumerating Amazon-S3, GCP and Azure buckets using given keyword/oganization-name... \")\n",
    "!python3 ../cloud_enum/cloud_enum.py -k $ORG_NAME -t 50 -l cloudenum.txt\n",
    "!cat cloudenum.txt |anew $FOLDER_NAME/Cloudenum.txt\n",
    "!rm cloudenum.txt\n",
    "print(\"Done. The file ./{}/Cloudenum.txt is updated!\".format(FOLDER_NAME))\n",
    "print(\"Note: Cloudenum.txt will not be part of final excel file!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df24a4-991a-4b14-b279-828e23d3e9ff",
   "metadata": {},
   "source": [
    "# Company Accounts\n",
    "CrossLinked: https://github.com/m8r0wn/CrossLinked\n",
    "\n",
    "Searching users on LinkedIn and creating email. \n",
    "\n",
    "For better results, run the following cell multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff77d87-8829-44b1-a5b0-24d2ef6a79f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if EMAIL_FORMAT != \"\":\n",
    "    print(\"Finding users on LinkedIn... \")\n",
    "    !python3 ../crosslinked/crosslinked.py -f $EMAIL_FORMAT@$DOMAIN $ORG_NAME -o crosslinked.csv\n",
    "    !cat crosslinked.csv |sort -u |anew $FOLDER_NAME/Emails.csv\n",
    "    !rm crosslinked.csv\n",
    "    print(\"Done. The file ./{}/Emails.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01c00b-0b21-4893-8d3c-38de79d8565e",
   "metadata": {},
   "source": [
    "## Breached database from Dehashed\n",
    "\n",
    "https://dehashed.com\n",
    "\n",
    "It will harvest credentials from breached databases, the Dehashed username and API key is required.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56be8e1-f94b-4823-b9eb-0a4b0375193b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash  -s \"$DOMAIN\" \"$DEHASHED_USER\" \"$DEHASHED_KEY\" \"$FOLDER_NAME\"\n",
    "if [ \"$3\" != \"\" ]; then\n",
    "echo \"Dumping breached databses from dehashed ...\"\n",
    "echo \"id, email, username, password, hashed_password, name, database_name\" >> $4/Dehashed.csv\n",
    "curl \"https://api.dehashed.com/search?query=domain:$1&size=4000\" -u $2:$3 -H 'Accept: application/json' | jq -r '.entries[] | {id: .id,email: .email,username: .username,password: .password,hashed_password: .hashed_password,name: .name,database_name: .database_name} | select((.password != null and .password!= \"\") )' | jq -r '[.[]] | @csv'|anew $4/Dehashed.csv\n",
    "echo \"Done. $4/Dehashed.csv is updated!\"\n",
    "echo \"Updating Email.csv with newly found Email addresses!\"\n",
    "grep -E -o \"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,6}\\b\" $4/Dehashed.csv |anew $4/Emails.csv\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f24f0-7f54-4a8f-a9a2-1a912b4efa13",
   "metadata": {},
   "source": [
    "# Excel and ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f09ed2-8c1b-4e6c-9df6-6196b0f4c64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo pip3 install pandas\n\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "#path to parse to and read files from\n",
    "path = \"/home/discovery/work/{}/\".format(FOLDER_NAME)\n",
    "\n",
    "#all files ending in .csv\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "#initialize writer\n",
    "writer = pd.ExcelWriter('/home/discovery/work/' + FOLDER_NAME + '/' + FOLDER_NAME + '_OSINT.xlsx', engine='xlsxwriter', options={'strings_to_formulas': False})\n",
    "\n",
    "#write all files into excel from dataframes and name worksheet by filename \n",
    "print(\"The following files are being parsed to \" + path + \":\") \n",
    "print(\"\")\n",
    "for f in all_files:\n",
    "    if os.stat(f).st_size == 0:\n",
    "        pass\n",
    "    else:\n",
    "        df = pd.read_csv(f)\n",
    "        print(f)\n",
    "        df.to_excel(writer, sheet_name=os.path.basename(f),index=False)\n",
    "\n",
    "writer.save()  \n",
    "\n",
    "print(\"\")\n",
    "print(\"Parsing of \" + FOLDER_NAME + \"_OSINT.xlsx Complete\")\n",
    "\n",
    "\n",
    "#delete csv files if excel creation was successful\n",
    "if os.path.exists('/home/discovery/work/' + FOLDER_NAME + '/' + FOLDER_NAME + '_OSINT.xlsx'):\n",
    "    !rm $FOLDER_NAME/*.csv\n",
    "\n",
    "#create zip file containing all the results\n",
    "import shutil\n",
    "shutil.make_archive('/home/discovery/work/' + FOLDER_NAME, 'zip', '/home/discovery/work/' + FOLDER_NAME)\n",
    "print(\"Results saved in \" + FOLDER_NAME + \".zip in ~/work\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
