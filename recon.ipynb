{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c1b991-ffc3-4a28-bb17-cf9d11d4a950",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Author: Ahsan Aziz - Security Consultant - The Missing Link\n",
    "# Version: 1.0 Date: 20/04/2022\n",
    "# Version: 2.0 Date: 09/04/2023\n",
    "# Inspired by Omar's BSides presentation: https://www.youtube.com/watch?v=LTNKMA65BtI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388e44bc-f5e0-4da8-999d-3838cb45fb93",
   "metadata": {},
   "source": [
    "# Description\n",
    "Automating OSINT (and a bit of scanning). If everything goes well, it will produce a zip file with all the information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73731d8-0d48-45cf-aa00-2c98a579da0f",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Prior to running any cells assign a value to the variables and run the cell. This will change the targets for enumeration without needing to modify the script parameters. \n",
    "\n",
    "If the Docker is built in a VM, set the adaptor to Bridged mode as some of the tools generate a lot of TCP connections and NAT interface can be easilty DoSed. \n",
    "\n",
    "It is suggested to run the notebook cell by cell instead of \"Run all cells\", troubleshooting will be easy and it will allow you stop a cell (if it takes longer) and move to the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e562ae-5482-4f75-b74b-559d25b9b9cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SCAN_TYPE = \"A\" #P=Passive, no interaction with the org's assets (except DNS requests). N=Normal, non-malicious TCP/HTTP requests sent. A=Aggressive, a number of malicious HTTP requests sent for vulnerability identification.\n",
    "WEB_SCAN = \"Y\" #Is web scanning required? (a bit noisy) Y or N.\n",
    "DOMAIN = \"tesla.com\"  #root domain (required)\n",
    "IP_RANGE = \"\" #nmap format. If empty, the IPs of subdomains will be used (optional)\n",
    "ORG_NAME = \"tesla\"  #this will be used in linkedIn search and cloud enumeration (required)\n",
    "FOLDER_NAME = \"tesla\" #this folder will contain the discovered recon data (required)\n",
    "VHOST = \"\" #domain for virtual host scanning, leave it empty if vhost scan is not required (optional)\n",
    "SSRF_URL = \"\" #provide a callback URL starting with https for SSRF checks (Optional)\n",
    "EMAIL_FORMAT = \"{f}{last}\" #e.g. {f}{last}, check hunter.io if you're not sure about the email format (optional)\n",
    "DEHASHED_USER = \"\" #dehashed username (optional)\n",
    "DEHASHED_KEY = \"\"  #dehashed API Key (optional)\n",
    "CHAOS_KEY = \"\" #CHAOS API key for subdomain enum, request your key: https://forms.gle/GP5nTamxJPfiMaBn9 (optional)\n",
    "CENSYS_KEY = \"\" #Censys API key for subdomain enum, request your key: https://censys.io/register (optional)\n",
    "CERTSPOTTER_KEY = \"\" #Certspotter API key for subdomain enum, request your key: https://sslmate.com/signup?for=certspotter_api (optional)\n",
    "SECURITYTRAILS_KEY = \"\" #SecurityTrails API key for subdomain enum, request your key: https://securitytrails.com/app/signup (optional)\n",
    "SHODAN_KEY = \"\" #Shodan API key for subdomain enum, request your key: https://account.shodan.io/login (optional)\n",
    "URLSCAN_KEY = \"\" #URLScan API key for subdomain enum, request your key: https://urlscan.io/user/signup (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea24eb-fa4b-4ca0-9c40-8813e2e3342f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Setting up project folders and files\")\n",
    "!mkdir $FOLDER_NAME\n",
    "!nmap -sL -n $IP_RANGE | grep 'Nmap scan report for' | cut -f 5 -d ' ' > $FOLDER_NAME/IP_Range.txt\n",
    "!mkdir $FOLDER_NAME/Screenshots\n",
    "print (\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f081cb1d-fc8d-4f96-bc1f-7dc2f2c0aaca",
   "metadata": {},
   "source": [
    "# NMAP\n",
    "All port scan is not included as it may take a while, add flags as per your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d330caa3-b91f-4d60-8126-55a454fbe087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IP_RANGE != \"\" and SCAN_TYPE != \"P\":\n",
    "        !nmap --open -Pn $IP_RANGE>> $FOLDER_NAME/nmap.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d960dc26-5c1b-4465-9908-f0fea2a9ff3d",
   "metadata": {},
   "source": [
    "# Subdomain Enumeration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d41c6-3a5a-4a01-9aec-031a1cfa29ea",
   "metadata": {},
   "source": [
    "## Subfinder\n",
    "subfinder: https://github.com/projectdiscovery/subfinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17481ddb-df4d-4517-90be-e05687073d34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!touch /home/discovery/.config/subfinder/config.yaml\n",
    "if 'provider-config.yaml' not in open(\"/home/discovery/.config/subfinder/config.yaml\").read():\n",
    "    #update config file with provider config\n",
    "    !echo \"provider-config: ~/.config/subfinder/provider-config.yaml\" >> ~/.config/subfinder/config.yaml\n",
    "    !echo \"\" > ~/.config/subfinder/provider-config.yaml\n",
    "\n",
    "    #update the API keys in provider config file\n",
    "    if CENSYS_KEY != \"\":\n",
    "        !echo \"censys:\" >> ~/.config/subfinder/provider-config.yaml\n",
    "        !echo \" - $CENSYS_KEY\" >> ~/.config/subfinder/provider-config.yaml\n",
    "    if CERTSPOTTER_KEY != \"\":\n",
    "        !echo \"certspotter:\" >> ~/.config/subfinder/provider-config.yaml\n",
    "        !echo \" - $CERTSPOTTER_KEY\" >> ~/.config/subfinder/provider-config.yaml\n",
    "    if SECURITYTRAILS_KEY != \"\":\n",
    "        !echo \"securitytrails:\" >> ~/.config/subfinder/provider-config.yaml\n",
    "        !echo \" - $SECURITYTRAILS_KEY\" >> ~/.config/subfinder/provider-config.yaml\n",
    "    if SHODAN_KEY != \"\":\n",
    "        !echo \"shodan:\" >> ~/.config/subfinder/provider-config.yaml\n",
    "        !echo \" - $SHODAN_KEY\" >> ~/.config/subfinder/provider-config.yaml\n",
    "    if URLSCAN_KEY != \"\":\n",
    "        !echo \"urlscan:\" >> ~/.config/subfinder/provider-config.yaml\n",
    "        !echo \" - $URLSCAN_KEY\" >> ~/.config/subfinder/provider-config.yaml\n",
    "\n",
    "print(\"Scraping domains using subfinder...\")\n",
    "!subfinder -d $DOMAIN -silent -o subfinder.csv >/dev/null 2>&1\n",
    "!cat subfinder.csv |anew -q $FOLDER_NAME/Subdomains.csv\n",
    "#anew: https://github.com/tomnomnom/anew\n",
    "!rm subfinder.csv\n",
    "print(\"Done. The file ./{}/Subdomains.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ec795-8096-4536-8131-afa25bca66e7",
   "metadata": {},
   "source": [
    "## Amass\n",
    "\n",
    "OWASP's Amass: https://github.com/OWASP/Amass \n",
    "\n",
    "Amass can also help you finding ASNs: amass intel -org tesla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc971a8b-38f5-4a42-b978-9375e9f9abfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SCAN_TYPE == \"A\":\n",
    "    print(\"Scraping domains using OWASP's amass...\")\n",
    "    !amass enum -d $DOMAIN -o amass.csv -r 8.8.8.8 -silent >/dev/null 2>&1\n",
    "    !cat amass.csv |anew -q $FOLDER_NAME/Subdomains.csv\n",
    "    !rm amass.csv\n",
    "    print(\"Done. The file ./{}/subdomains.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75c69ac-8eed-4c56-b814-7eaff016552f",
   "metadata": {},
   "source": [
    "## DNS database search - CHAOS\n",
    "Chaos: https://chaos.projectdiscovery.io/#/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55698fb-bad7-4bcc-8638-8021cbb7f0a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CHAOS_KEY != \"\":\n",
    "    print(\"Querying CHAOS...\")\n",
    "    !chaos -key $CHAOS_KEY -d $DOMAIN -silent -o chaos.csv >/dev/null 2>&1\n",
    "    !cat chaos.csv |dnsx -silent|anew -q $FOLDER_NAME/Subdomains.csv\n",
    "    !rm chaos.csv\n",
    "    print(\"Done. The file ./{}/Subdomains.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931a868e-5853-4aaa-9beb-40d6a7513d60",
   "metadata": {},
   "source": [
    "## Subdomain Bruteforcing\n",
    "shuffledns: https://github.com/projectdiscovery/shuffledns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aac7e7-018a-43c8-b017-b8aa8d6c8f07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SCAN_TYPE == \"A\":\n",
    "    print(\"Bruteforcing using shuffledns with commonspeak wordlist... \")\n",
    "    !shuffledns -silent -d $DOMAIN -w ../commonspeak2.txt -r ../resolvers.txt >> shuffledns.csv\n",
    "    !cat shuffledns.csv |anew -q $FOLDER_NAME/Subdomains.csv\n",
    "    !rm shuffledns.csv\n",
    "    print(\"Done. The file ./{}/Subdomains.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cefc17-3232-4c3c-ad91-0404d14a426b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Resolving domains\n",
    "\n",
    "Checking if all subdomains resolve to IP addresses using dnsx.\n",
    "\n",
    "dnsx: https://github.com/projectdiscovery/dnsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325518b0-aa73-426a-84ae-862db0f5a27d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Creating the csv with two columns (all domains and the corresponding IP address)...\")\n",
    "!cp $FOLDER_NAME/Subdomains.csv tmp.csv\n",
    "for i in range(3):\n",
    "    !cat Subdomains.txt |dnsx -silent -a -resp -nc|sed -e 's/\s\+/,/g'|anew output.csv\n",
    "!cat output.csv |anew -q $FOLDER_NAME/Subdomains_IPs.csv\n",
    "!rm output.csv && rm tmp.csv\n",
    "print(\"Done. The file ./{}/Subdomains_IPs.csv is updated!\".format(FOLDER_NAME)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2258d83-2adc-4965-ae2a-4b071caa7d9e",
   "metadata": {},
   "source": [
    "## In-scope Domains\n",
    "Filtering subdomains as per IP_RANGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f6a8b7-53f6-47cf-8f29-87ff03d69409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IP_RANGE != \"\":\n",
    "    print(\"Let's filter domains from Subdomains_IPs as per IP_RANGE...\")\n",
    "    !cp $FOLDER_NAME/Subdomains_IPs.csv subs.txt && cp $FOLDER_NAME/IP_Range.txt ip.txt && touch tmp.csv\n",
    "    !cat ip.txt |grep -Ff - subs.txt |anew -q tmp.csv\n",
    "    !rm ip.txt && rm subs.txt\n",
    "    !cat tmp.csv |anew -q $FOLDER_NAME/In_Scope_Subdomains.csv && rm tmp.csv\n",
    "    print(\"Done. The file ./{}/In_Scope_Subdomains.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6336dc7d-e078-496c-8ffb-b2139efaf4c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Shodoan's Nrich\n",
    "\n",
    "A command-line tool to quickly analyze all IPs in a file and see which ones have open ports/ vulnerabilities. \n",
    "\n",
    "nrich: https://gitlab.com/shodan-public/nrich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac07c0-586d-4083-9645-a829d6ae484f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IP_RANGE != \"\":\n",
    "    print(\"Querying shodan's database for IP_RANGE provided...\")\n",
    "    !cp $FOLDER_NAME/IP_Range.txt ip.txt\n",
    "else:\n",
    "    print(\"Querying shodan's database for Subdomains_IPs...\")\n",
    "    !cp $FOLDER_NAME/Subdomain_IPs.csv subs.txt\n",
    "    !cat subs.txt |grep -oE \"\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b\"|anew -q ip.txt\n",
    "\n",
    "!cat ip.txt |nrich - >> $FOLDER_NAME/Shodan_Nrich.txt \n",
    "!rm ip.txt    \n",
    "print(\"Done. The file ./{}/Shodan_Nrich.txt is updated!\".format(FOLDER_NAME)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d45c2e-f7b3-4150-989d-dcae082c7668",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Subdomain Takeover\n",
    "\n",
    "dnsReaper: https://github.com/punk-security/dnsReaper\n",
    "\n",
    "##### Note: Since the cloud service providers keep updating their subdomain policies, there is no reliable automated tool to confirm subdomain takeover. dnsReaper may provide false positives. To get the latest information about subdomain takeover, head over to: https://github.com/EdOverflow/can-i-take-over-xyz/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124ba5ca-17a7-42b5-9228-daa866cc2107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Running dnsreaper on all subdomains (and not just the domains in scope)...\")\n",
    "!cp $FOLDER_NAME/Subdomains.csv subs.csv\n",
    "!python3.10  ../dnsReaper/main.py file --filename subs.csv --out Subdomain_Takeover.csv \n",
    "!rm subs.csv\n",
    "!cp Subdomain_Takeover.csv $FOLDER_NAME/Subdomain_Takeover.csv\n",
    "!rm Subdomain_Takeover.csv\n",
    "print(\"Done. The file ./{}/Subdomain_Takeover.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e30296-8568-4352-b9f9-320a5eee1b57",
   "metadata": {
    "tags": []
   },
   "source": [
    "# HTTP Probing\n",
    "Probing in-scope domains using httpx: https://github.com/projectdiscovery/httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410cbf8c-cfd5-46c3-8eac-f25c8e197d8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SCAN_TYPE != \"P\":\n",
    "    print(\"Probing using httpx...\")\n",
    "    if IP_RANGE == \"\":   \n",
    "        !cp $FOLDER_NAME/Subdomains.csv subs.csv\n",
    "    else:\n",
    "        !cp $FOLDER_NAME/In_Scope_Subdomains.csv subs.csv       \n",
    "    !cat subs.csv |dnsx -silent |httpx -silent >> httpx.csv\n",
    "    !cat httpx.csv |anew -q $FOLDER_NAME/Probed_Subdomains.csv\n",
    "    print(\"Done. The file ./{}/Probed_Subdomains.csv is updated!\".format(FOLDER_NAME))\n",
    "    !rm subs.csv\n",
    "    !rm httpx.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df4c86-c7a9-45b4-ae4b-57c380d4f460",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Screenshoting\n",
    "\n",
    "Eyewitness: https://github.com/FortyNorthSecurity/EyeWitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb7afb-d91f-44d6-b74f-8e6bf7a94b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$SCAN_TYPE\" \"$FOLDER_NAME\" \n",
    "if [ \"$1\" != \"P\" ]; then\n",
    "echo \"Screenshoting probed domians using EyeWitness...\"\n",
    "cp $2/Probed_Subdomains.csv subs.csv\n",
    "python3 ../EyeWitness/Python/EyeWitness.py --delay 3 --no-prompt -f subs.csv -d $2/Screenshots/\n",
    "echo \"Done. The screenshots for Probed_Subdomains are saved in ./$2/screenshots/report.html\"\n",
    "rm subs.csv\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6238794d-73e6-4938-bd7c-d41e9b986bfc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# VHOST Scanning \n",
    "If a reverse proxy like nginx is in use, some subdomains may not have DNS entries; using a fuzzer (ffuf in this case) we can try bruteforcing the virtual hosts with the host header. It may produce false positives as some web servers respond to all hosts.\n",
    "\n",
    "FFUF: https://github.com/ffuf/ffuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722834d0-6d2b-4542-b213-a974523dbde7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SCAN_TYPE == \"A\" and VHOST != \"\":\n",
    "    print(\"Bruteforcing vhosts using commonspeak wordlist. ... \")\n",
    "    !ffuf -s -u \"https://$VHOST\" -w ../commonspeak2.txt -H \"Host: FUZZ.$DOMAIN\" -of csv -o ffuf.csv\n",
    "    !cat ffuf.csv |sort -u |anew -q $FOLDER_NAME/Vhosts.csv\n",
    "    !rm ffuf.csv\n",
    "    print(\"Done. The file ./{}/Vhosts.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df24a4-991a-4b14-b279-828e23d3e9ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Company Accounts\n",
    "CrossLinked: https://github.com/m8r0wn/CrossLinked\n",
    "\n",
    "Searching users on LinkedIn and creating emails. \n",
    "\n",
    "For better results, run the following cell multiple times. Try different permutations of the company name, check company's linkedin page and try that name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff77d87-8829-44b1-a5b0-24d2ef6a79f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if EMAIL_FORMAT != \"\":\n",
    "    print(\"Finding users on LinkedIn... \")\n",
    "    !python3 ../crosslinked/crosslinked.py -f $EMAIL_FORMAT@$DOMAIN $ORG_NAME -o crosslinked\n",
    "    !cat crosslinked.txt |sort -u |anew -q $FOLDER_NAME/Emails.csv\n",
    "    !rm crosslinked.txt\n",
    "    print(\"Done. The file ./{}/Emails.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a01c00b-0b21-4893-8d3c-38de79d8565e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Breached database from Dehashed\n",
    "\n",
    "https://dehashed.com\n",
    "\n",
    "It will harvest credentials from breached databases, the Dehashed username and API key is required.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56be8e1-f94b-4823-b9eb-0a4b0375193b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash  -s \"$DOMAIN\" \"$DEHASHED_USER\" \"$DEHASHED_KEY\" \"$FOLDER_NAME\"\n",
    "if [ \"$3\" != \"\" ]; then\n",
    "echo \"Dumping breached databses from dehashed ...\"\n",
    "echo \"id, email, username, password, hashed_password, name, database_name\" >> $4/Dehashed.csv\n",
    "curl \"https://api.dehashed.com/search?query=domain:$1&size=4000\" -u $2:$3 -H 'Accept: application/json' | jq -r '.entries[] | {id: .id,email: .email,username: .username,password: .password,hashed_password: .hashed_password,name: .name,database_name: .database_name} | select((.password != null and .password!= \"\") )' | jq -r '[.[]] | @csv'|anew -q $4/Dehashed.csv\n",
    "echo \"Done. $4/Dehashed.csv is updated!\"\n",
    "echo \"Updating Email.csv with newly found Email addresses!\"\n",
    "grep -E -o \"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,6}\\b\" $4/Dehashed.csv |anew -q $4/Emails.csv\n",
    "echo \"Done. Email.csv is updated!\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadaa9fe-36ea-402c-822e-ec220ba95982",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cloud Storage Misconfigurations\n",
    "Cloudenum: https://github.com/initstring/cloud_enum\n",
    "\n",
    "If no bucket is found, try with different keywords.\n",
    "\n",
    "### AWS cli commands:\n",
    "```\n",
    "aws s3 ls s3://{bucket} --no-sign-request\n",
    "aws s3 cp abc.txt s3://{bucket}/abc.txt --no-sign-request\n",
    "aws s3 rm s3://{bucket}/abc.txt --no-sign-request\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd9e814-d47f-47a8-8a3a-7ebf2d5606a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Enumerating Amazon-S3, GCP and Azure buckets using given keyword/oganization-name... \")\n",
    "!python3 ../cloud_enum/cloud_enum.py -k $ORG_NAME -t 50 -l cloudenum.txt >/dev/null 2>&1\n",
    "!cat cloudenum.txt |anew -q $FOLDER_NAME/Cloudenum.txt\n",
    "!rm cloudenum.txt\n",
    "print(\"Done. The file ./{}/Cloudenum.txt is updated!\".format(FOLDER_NAME))\n",
    "print(\"Note: Cloudenum.txt will not be part of final excel file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3034d38-8bb7-4368-bd30-8cf38d31efee",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Meta Data from PDF and Office Documents\n",
    "\n",
    "Pymeta extracts meta data from office files.\n",
    "\n",
    "It uses specially crafted search queries to identify and download the following file types (pdf, xls, xlsx, csv, doc, docx, ppt, pptx) from a given domain using Google and Bing scraping. It can be helpful to find: domains, user accounts, naming conventions, software/version numbers, and more!\n",
    "\n",
    "pymeta: https://github.com/m8r0wn/pymeta\n",
    "\n",
    "**Note: Currently, this tool isn't working with Python3.10 (https://github.com/m8sec/pymeta/issues/20). You can still try it, author may have fixed the issue.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd1c9d3-ad12-4cf1-b82c-b0b40f31e806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SCAN_TYPE != \"P\":\n",
    "    print(\"Running pymeta. ... \")\n",
    "    !pymeta -d $DOMAIN -f $FOLDER_NAME/Pymeta.csv -o $FOLDER_NAME\n",
    "    print(\"Done. The file ./{}/Pymeta.csv is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd53a20a-4445-44b8-a29b-f2194d666259",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Search for txt record - SMTP SPF\n",
    "\n",
    "Use this for domain scanning: https://caniphish.com/free-phishing-tools/email-spoofing-test\n",
    "\n",
    "Check this blog out for details: https://caniphish.com/phishing-resources/blog/compromised-australian-email-infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe5f7bf-6858-4d59-9e1d-01f1108018a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Web Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e48e6c-c11e-4f0b-9f33-585a7d0dee15",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Nuclei Scan\n",
    "Nuclei is a web scanner, it can detect technologies in use, identify CORS and TLS issues, and can scan for famous zero days such as log4j. Give it a go, it's pretty good!\n",
    "\n",
    "**This might take sometime depending on the number of domains. It is a bit noisey, may send hundreds of GET/POST requests, be careful in a red team engagement.**\n",
    "\n",
    "Nuclei: https://github.com/projectdiscovery/nuclei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abcc504-40e7-4b37-a437-748f2dac9c27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SCAN_TYPE == \"A\" and WEB_SCAN == \"Y\":\n",
    "    print(\"Scanning probed domains using nuclei...\")\n",
    "    !nuclei -update -silent\n",
    "    !nuclei -update-templates -silent #let's first update nuclei database\n",
    "    !cp $FOLDER_NAME/Probed_Subdomains.csv subs.csv\n",
    "    !nuclei -l subs.csv -silent >> $FOLDER_NAME/Probed_Subdomains_Nuclei.txt\n",
    "    print(\"Done. Nuclei resuts saved in  ./{}/Probed_Subdomains_Nuclei.txt\".format(FOLDER_NAME))\n",
    "    !rm subs.csv        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44839db-3d96-48d7-9ab7-e8810d8fd2b1",
   "metadata": {},
   "source": [
    "### HTTP Request Smuggling Scanner\n",
    "\n",
    "Smuggler: https://github.com/defparam/smuggler\n",
    "\n",
    "Note that this runs on all subdomains (in-scope if applicable). If multiple subdomains are resolving to the same IP address, you may want to pick one of them and run agains it. Also be careful in a production environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38adb83-bd1a-45ff-a999-24393fdb7cf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$FOLDER_NAME\" \"$SCAN_TYPE\" \"$WEB_SCAN\"\n",
    "if [ \"$2\" != \"P\" ] && [ \"$3\" == \"Y\" ]; then\n",
    "echo \"Running smuggler on probed domians using Smuggler...\"\n",
    "cp $1/Probed_Subdomains.csv subs.csv\n",
    "cat subs.csv | python3 ../smuggler/smuggler.py >> $1/smuggler.txt\n",
    "echo \"Done. The file ./$1/Pobed_Smuggler.txt is updated\"\n",
    "rm subs.csv\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70076060-e06b-4a77-a123-2537dfb60be8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Wayback Machine\n",
    "\n",
    "Get list of URLs, current and old, from the web archive.\n",
    "\n",
    "If you want to avoid crawling the website or directory bruteforcing, try this. You may find a hidden functionality, i.e. a functionality used to exist in older versions but the link is removed from the current one. Good stuff! Might fetch a lot of URLs depending on the number of domains.\n",
    "\n",
    "waybackurls: https://github.com/tomnomnom/waybackurls\n",
    "\n",
    "Also checkout: https://github.com/xnl-h4ck3r/waymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5043f3f-c489-45ac-ada8-8674e685fc68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SCAN_TYPE == \"A\" and WEB_SCAN == \"Y\":\n",
    "    print(\"Running wayback urls on the root domain. ... \")\n",
    "    !waybackurls $DOMAIN |sed -r '/^\\s*$/d'|anew -q urls.txt\n",
    "    if IP_RANGE != \"\":   \n",
    "        print(\"Filtering results with in-scope subdomains. ... \")\n",
    "        !cp $FOLDER_NAME/Probed_Subdomains.csv subs.csv\n",
    "        !cat subs.csv |grep -Ff - urls.txt |anew -q inscope-urls.txt\n",
    "        !mv inscope-urls.txt urls.txt && rm subs.csv\n",
    "\n",
    "    #add https version of URLs also\n",
    "    !sed 's/http:/https:/g' urls.txt|anew -q urls.txt\n",
    "    !cat urls.txt |anew -q $FOLDER_NAME/Subdomains_urls.txt && rm urls.txt\n",
    "    print(\"Done. The file ./{}/Subdomains_urls.txt is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0152257-2fdc-4805-9a98-e6c2be6cecdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Crawling\n",
    "\n",
    "Katana: https://github.com/projectdiscovery/katana\n",
    "\n",
    "It will crawl with the depth of 3, increase or decrease according to your requirements. It will also crawl URLs found in the JS files.\n",
    "\n",
    "The output of this will be merged with the URLs found using the wayback machine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553deab-91aa-4346-a79a-d005cd7ac1a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SCAN_TYPE == \"A\" and WEB_SCAN == \"Y\":\n",
    "    print(\"Running katana on subdomains. ... \")\n",
    "    !cp $FOLDER_NAME/Probed_Subdomains.csv subs.csv\n",
    "    !katana -list subs.csv -jc -d 3 -silent|sed -r '/^\\s*$/d' |anew -q $FOLDER_NAME/Subdomains_urls.txt\n",
    "    print(\"Done. The file ./{}/Subdomains_urls.txt is updated!\".format(FOLDER_NAME))\n",
    "    !rm subs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e15df5-57c5-4c9c-b046-b5e22b39aef1",
   "metadata": {},
   "source": [
    "### Remove Duplicate URLs\n",
    "\n",
    "uro: https://github.com/s0md3v/uro\n",
    "\n",
    "The urls list, especially from the Waybackmachine, may have duplicate URLs, e.g. https://test.com/h?q=foo and https://test.comj?q=bar are same URL. This tool will remove the duplicates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2c9b94-249d-420d-95e4-5ea409ae1f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Running deduplicate on subdomains. ... \")\n",
    "!cat $FOLDER_NAME/Subdomains_urls.txt | uro |anew -q $FOLDER_NAME/Dedupe_Subdomains_urls.txt\n",
    "#!cat $FOLDER_NAME/Subdomains_urls.txt | deduplicate --hide-useless --sort |anew -q $FOLDER_NAME/Dedupe_Subdomains_urls.txt\n",
    "print(\"Done. The file ./{}/Dedupe_Subdomains_urls.txt is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa62642-b903-4351-b043-491038f1b1ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NPM Dependency Confusion Sacanning\n",
    "\n",
    "Checkout this research article for more info: https://medium.com/@alex.birsan/dependency-confusion-4a5d60fec610\n",
    "\n",
    "The followign script is modified form of: https://github.com/x1337loser/Dependency-Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b618e-f504-4681-bb5a-3aaf8ba5f42f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$FOLDER_NAME\" \"$SCAN_TYPE\" \"$WEB_SCAN\"\n",
    "\n",
    "if [ \"$2\" != \"P\" ] && [ \"$3\" == \"Y\" ]; then\n",
    "echo \"Running the dependency confusion scan for npm packages...\"\n",
    "if [ -d $1/dependency ];then\n",
    "    echo '' >/dev/null 2>&1\n",
    "else \n",
    "    mkdir $1/dependency;\n",
    "fi\n",
    "\n",
    "echo -e \"Getting the waybackurls output and searching of JS files....\"\n",
    "cp $1/Dedupe_Subdomains_urls.txt urls.txt\n",
    "\n",
    "cat urls.txt | sort -u | grep .js | sed 's/?.*//' | grep -v '/wp-content/\\|/wp-includes/\\|.json\\|jpg\\|png\\|css|\\|/member/\\|.jsp\\|oauth\\|login\\|en-us\\|v=\\|=\\|?\\|/help/\\|/id/\\|paragon\\|/wp-json/' | sort -u | anew $1/dependency/js-urls.txt >/dev/null 2>&1;\n",
    "rm urls.txt\n",
    "\n",
    "echo -e \"Found $(cat $1/dependency/js-urls.txt | sort -u |wc -l) js file url \";\n",
    "cat $1/dependency/js-urls.txt | sort -u |while read ut;do\n",
    "    wget $ut.map -o $1/dependency/ >/dev/null 2>&1;\n",
    "    done\n",
    "    \n",
    "grep -oriahE \"[^\\\"\\\\'> ]+\" $1/dependency/ | grep 'node_modules' | grep -v '@' | sed 's:.*/node_modules::' | cut -d '/' -f 2 | sort -u | grep -v '.js\\|.ts\\|.tsx\\|.css' | egrep '\\b[a-z]+\\b' | grep -v '.png\\|.pnp' | anew $1/dependency/npm-packages.txt >/dev/null 2>&1;\n",
    "\n",
    "rm $1/dependency/js-urls.txt;\n",
    "if [ -s $1/dependency/npm-packages.txt ];then\n",
    "    echo -e \"   Found some packages now going for final test on \"$1/dependency/npm-packages.txt\"\";\n",
    "    cat $1/dependency/npm-packages.txt | sort -u | while read ut;do\n",
    "        if $(curl -o /dev/null -s -w \"%{http_code}\\n\" \"https://registry.npmjs.org/$ut\" | grep \"404\" >/dev/null 2>&1); then\n",
    "            echo -e \"\"$ut\" \\e[1;31mFound Private npm packgae, \\e[0m\" && echo $ut >> $1/npm-vuln.txt;\n",
    "\n",
    "        else\n",
    "            echo -e \"\"$ut\"\\e[1;33m Available in Public Registry \\e[0m\";\n",
    "        fi\n",
    "        done\n",
    "else\n",
    "    echo -e \"Didn't find any npm packages, now going for scope test \"\n",
    "fi\n",
    "#this part is for scope package test please be carefull with that, some times `www.npmjs.com` will show you 429 response code\n",
    "grep -oriahE \"[^\\\"\\\\'> ]+\" $1/dependency/ | grep 'node_modules' | sed 's:.*/node_modules::' | cut -d '/' -f 2 | sort -u | grep '@' | grep -v '.js\\|.ts\\|.tsx\\|.css' | egrep '\\b[a-z]+\\b' | grep -v '.png\\|.pnp' | grep '@' | cut -d '@' -f 2 | anew $1/dependency/npm-scope.txt >/dev/null 2>&1;\n",
    "\n",
    "if [ -s $1/dependency/npm-packages.txt ];then\n",
    "    echo -e \"   Found some Scope names now going for final test on \"$1/dependency/npm-scope.txt\"\";\n",
    "    cat $1/dependency/npm-scope.txt | sort -u | while read pkg;do\n",
    "    OPTION=`curl -o /dev/null -s -w \"%{http_code}\\n\" \"https://www.npmjs.com/org/$pkg\"`\n",
    "        if $(echo \"$OPTION\" | grep \"200\\|302\" >/dev/null 2>&1);then\n",
    "            echo -e \"@\"$pkg\"\\e[1;33m Available in Public Registry \\e[0m\" && echo $pkg >> $1/npm-scope-vuln.txt;\n",
    "            grep -oriahE \"[^\\\"\\\\'> ]+\" $1/dependency/ | grep 'node_modules' |grep '@'$pkg'' | sed 's:.*/@'$pkg'::' | cut -d '/' -f 2 | sort -u | while read ut;do echo \"Full pacakge name of @\"$pkg\" is @\"$pkg\"/\"$ut\" \";done\n",
    "        elif $(echo \"$OPTION\" | grep \"429\" >/dev/null 2>&1);then\n",
    "            echo -e \"@\"$pkg\" \\e[1;31m Rate limit detected \\e[0m\"\n",
    "\n",
    "        else\n",
    "            echo -e \"@\"$pkg\"\\e[1;31m Found Unclaimed scope Name\\e[0m\";\n",
    "            grep -oriahE \"[^\\\"\\\\'> ]+\" $1/dependency/| grep 'node_modules' |grep '@'$pkg'' | sed 's:.*/@'$pkg'::' | cut -d '/' -f 2 | sort -u | while read ut;do echo -e \"\\e[1;31mFull pacakge name of @\"$pkg\" is @\"$pkg\"/\"$ut\", this is unclaimed, Add @\"$pkg\"/\"$ut\" in your package.json file like {package: @\"$pkg\"/\"$ut\"}, \\e[0m\";done\n",
    "        fi\n",
    "        done\n",
    "else\n",
    "    echo -e \"Didn't find any Scope name\";\n",
    "fi\n",
    "fi\n",
    "echo \"Done.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a676a-5355-4674-8d16-def98dbec25f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Python, PHP, PERL, MAVEN and NPM Dependency Confusion Scanning\n",
    "\n",
    "Confused: https://github.com/visma-prodsec/confused\n",
    "\n",
    "This will use URLs from Wayback machine and Crawling. If you have the URLs for the config files (package.json, pm.xml etc.) and want to supply your own input, create a file called url.txt in the current directory and run this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e79d3b-69ec-48e4-8ff2-963d1d43f293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$FOLDER_NAME\" \"$SCAN_TYPE\" \"$WEB_SCAN\"\n",
    "if [ \"$2\" != \"P\" ] && [ \"$3\" == \"Y\" ]; then\n",
    "echo \"Running confused on the urls collected in previous steps. ......\"\n",
    "cp $1/Dedupe_Subdomains_urls.txt dedupe.txt\n",
    "\n",
    "    #find depdency config files in the waybackurl\n",
    "grep -i -E 'package.json|package-lock.json|requirements.txt|composer.json|pom.xml|gemfile.lock' dedupe.txt |anew -q urls.txt\n",
    "rm dedupe.txt\n",
    "\n",
    "if [ -s \"urls.txt\" ];then\n",
    "    mkdir dependency2\n",
    "    echo \"Found files to scan!\"\n",
    "    while read -r line; do echo $line; wget $line -P dependency2/ --user-agent=\"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36\" >/dev/null 2>&1;\n",
    "        if [ -e dependency2/package.json ]; then\n",
    "            echo \"A package.json was found at $line\";\n",
    "            confused -l npm dependency2/package.json >> $1/dependency-confusion.txt\n",
    "            rm dependency2/package.json;\n",
    "        elif [ -e \"dependency2/package-lock.json\" ]; then \n",
    "            echo \"A package-lock.json was found at $line\";\n",
    "            confused -l npm dependency2/package-lock.json >> $1/dependency-confusion.txt\n",
    "            rm dependency2/package-lock.json;\n",
    "        elif [ -e \"dependency2/requirements.txt\" ]; then \n",
    "            echo \"A requirements.txt was found at $line\";\n",
    "            confused -l pip dependency2/requirements.txt >> $1/dependency-confusion.txt;\n",
    "            rm dependency2/requirements.txt\n",
    "        elif [ -e \"dependency2/composer.json\" ]; then \n",
    "            echo \"A composer.json was found at $line\";\n",
    "            confused -l php dependency2/composer.json >> $1/dependency-confusion.txt;\n",
    "            rm dependency2/composer.json\n",
    "        elif [ -e \"dependency2/pom.xml\" ]; then \n",
    "            echo \"A pom.xml was found at $line\";\n",
    "            confused -l mvn dependency2/pom.xml >> $1/dependency-confusion.txt;\n",
    "            rm dependency2/pom.xml\n",
    "        elif [ -e \"dependency2/gemfile.lock\" ]; then \n",
    "            echo \"A gemfile.lock was found at $line\";\n",
    "            confused -l rubygems dependency2/gemfile.lock >> $1/dependency-confusion.txt;\n",
    "            rm dependency2/gemfile.lock\n",
    "        fi;\n",
    "        rm dependency2 -R;\n",
    "    done < urls.txt\n",
    "rm urls.txt\n",
    "fi\n",
    "echo \"Done. $1/dependency-confusion.txt is updated\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba85240-ac4f-4bc7-ba4f-6411a590b857",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Secrets in JS/JSON Files\n",
    "\n",
    "This will look for secrets e.g. AWS creds or API keys in the JS and JSON files identified in the previous steps. If you know the git repo of the organization, you can scan it by providing the org name in the arguments. Check trufflehog's help for more details. \n",
    "\n",
    "Trufflehog: https://github.com/trufflesecurity/trufflehog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb5d3b-12f7-41ea-9bc8-b4621a0da240",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Download js/json/html files\n",
    "if SCAN_TYPE != \"P\" and WEB_SCAN == \"Y\":\n",
    "    !cp $FOLDER_NAME/Dedupe_Subdomains_urls.txt urls.txt\n",
    "    !cat urls.txt | sort -u | grep \"\\.js\" |anew -q $FOLDER_NAME/js-urls.txt\n",
    "    !cat urls.txt | sort -u | grep \"\\.html\" |anew -q $FOLDER_NAME/html-urls.txt\n",
    "    !mkdir jsfiles && mkdir htmlfiles && rm urls.txt\n",
    "    !mkdir $FOLDER_NAME/jsfiles && mkdir $FOLDER_NAME/htmlfiles\n",
    "    !cp $FOLDER_NAME/js-urls.txt js-urls.txt && cp $FOLDER_NAME/js-urls.txt html-urls.txt\n",
    "    print(\"downloading js/json files\")\n",
    "    !cat js-urls.txt |fff -s 200 -o jsfiles/ >/dev/null 2>&1\n",
    "    print(\"downloading html files\")\n",
    "    !cat html-urls.txt |fff -s 200 -o htmlfiles/ >/dev/null 2>&1\n",
    "    !mv jsfiles/* $FOLDER_NAME/jsfiles/ && mv htmlfiles/* $FOLDER_NAME/htmlfiles/\n",
    "    !rm jsfiles -R && rm htmlfiles -R\n",
    "    !rm js-urls.txt && rm html-urls.txt\n",
    "    print(\"Done downloading. The jsfiles and htmlfiles directories are created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdff51d-5453-4337-ba22-f8ae14769612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trufflehog\n",
    "if SCAN_TYPE != \"P\" and WEB_SCAN == \"Y\":\n",
    "    print(\"Running trufflehog on JS/JSON files. ... \")\n",
    "    !trufflehog filesystem $FOLDER_NAME/jsfiles/ >> $FOLDER_NAME/trufflehog.txt\n",
    "    print(\"Running trufflehog on html files. ... \")\n",
    "    !trufflehog filesystem $FOLDER_NAME/htmlfiles/ >> $FOLDER_NAME/trufflehog.txt\n",
    "    print(\"Done. The file ./{}/trufflehog.txt is updated!\".format(FOLDER_NAME))\n",
    "\n",
    "#secrets using gf\n",
    "if SCAN_TYPE != \"P\" and WEB_SCAN == \"Y\":\n",
    "    print(\"Secrets in JS/JSONE files...\")\n",
    "    !mkdir jsfiles && cp $FOLDER_NAME/jsfiles/* jsfiles/ -r\n",
    "    !mkdir htmlfiles && cp $FOLDER_NAME/htmlfiles/* htmlfiles/ -r\n",
    "    !for i in `~/go/bin/gf -list`; do ~/go/bin/gf ${i} jsfiles/ |anew -q Potential_Secrets.txt; done\n",
    "    print(\"Secrets in HTML files...\")\n",
    "    !for i in `~/go/bin/gf -list`; do ~/go/bin/gf ${i} htmlfiles/ |anew -q Potential_Secrets.txt; done\n",
    "    !mv Potential_Secrets.txt $FOLDER_NAME/Potential_Secrets.txt && rm jsfiles -r && rm htmlfiles -r\n",
    "    print(\"Done. The file ./{}/Potential_Secrets.txt is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aebe8f-91d1-4441-a8d3-93e0c5d712e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#manually grepping strings in js/json/html files, customize it according to your needs\n",
    "if SCAN_TYPE != \"P\" and WEB_SCAN == \"Y\":\n",
    "    #!grep -E '\"username\" :|\"username\":|username:|user:' $FOLDER_NAME/jsfiles/ -orl|anew -q $FOLDER_NAME/Potential_Secrets.txt\n",
    "    !grep -E '\"password\" :|\"password\":|password:|pwd:|pass:' $FOLDER_NAME/jsfiles/ -orl|anew -q $FOLDER_NAME/Potential_Secrets.txt\n",
    "    !grep -E '\"password\" :|\"password\":|password:|pwd:|pass:' $FOLDER_NAME/htmlfiles/ -orl|anew -q $FOLDER_NAME/Potential_Secrets.txt\n",
    "    print(\"Done. The file ./{}/Potential_Secrets.txt is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea697b1-36a8-415c-83e2-5777442dab6d",
   "metadata": {},
   "source": [
    "### AWS Cognito\n",
    "\n",
    "Using keywords e.g. IdentityPoolId, the following cell check if AWS Cognito is in use. Exposing the IdentityPoolId is not a vulnerability on its own, but if we can check if the privileges are misconfigured. Here are the steps (requires AWS CLI):\n",
    "\n",
    "* Grab the IdentityPoolId from the identified file\n",
    "* Use the command ```aws cognito-identity get-id --identity-pool-id <IdentityPoolId> --region <region>``` to fetch the IdentityId.\n",
    "* Use IdentityId to generate temporary AWS credentials: ```aws cognito-identity get-id get-credentials-for-identity --identity-id <IdentityId> --region <region>```\n",
    "* Use the python script or ScoutSuite to enumerate privileges of the credentials and see if it has any abnormal privileges (e.g. S3 buckets, Dynamo DB etc.) ```./enumerate_iam.py --access-key <AccessKeyID> --secret-key <SecretKey> --session-key <SessionKey>```\n",
    "* Check if you're able to talk to Cognito API, e.g. if user registration is not enabled, try this to register user: ```aws cognito-idp sign-up --client-id <ClientID> --username <username> --password <password> --region <region>```\n",
    "* Run ScouteSuite to get all the privileges of the AWS credentials.\n",
    "\n",
    "\n",
    "\n",
    "enumerate_iam: https://github.com/andresriancho/enumerate-iam\n",
    "\n",
    "ScouteSuite: https://github.com/nccgroup/ScoutSuite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1d720c-3c0d-4e3f-a6e4-f1564e6c9e54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SCAN_TYPE != \"P\" and WEB_SCAN == \"Y\":\n",
    "    !grep -E 'Aws_cognito_identity_pool_id |identityPoolId|userPoolWebClientId|userPoolId|aws_user_pools_id' $FOLDER_NAME/jsfiles/ -orl|anew -q $FOLDER_NAME/AWS_Cognito.txt\n",
    "    !grep -E 'Aws_cognito_identity_pool_id |identityPoolId|userPoolWebClientId|userPoolId|aws_user_pools_id' $FOLDER_NAME/htmlfiles/ -orl|anew -q $FOLDER_NAME/AWS_Cognito.txt\n",
    "    print(\"Done. The file ./{}/AWS_Cognito.txt is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e53fc-24a8-4af0-8d4e-2a7510983d9a",
   "metadata": {},
   "source": [
    "### S3 Buckets \n",
    "\n",
    "The following will find S3 buckets from JS/JSON/HTML Files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020e2dff-e4a1-4be8-95a8-72aebeaa0c47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SCAN_TYPE != \"P\" and WEB_SCAN == \"Y\":\n",
    "    !gf s3-buckets $FOLDER_NAME/htmlfiles/ | anew -q $FOLDER_NAME/S3-buckets.txt\n",
    "    !gf s3-buckets $FOLDER_NAME/jsfiles/ | anew -q $FOLDER_NAME/S3-buckets.txt\n",
    "    #following commands are for manual grepping, haevn't fully tested gf so keeping the manual part for just in case\n",
    "    #!grep -r \"amazonaws.com\" $FOLDER_NAME/htmlfiles/ | grep -Eo \"(http|https)://[a-zA-Z0-9./?=_%:-]*\" |grep \"s3\" |grep \"amazonaws.com\"  |anew -q $FOLDER_NAME/S3-buckets.txt\n",
    "    #!grep -r \"amazonaws.com\" $FOLDER_NAME/jsfiles/ | grep -Eo \"(http|https)://[a-zA-Z0-9./?=_%:-]*\" |grep \"s3\" |grep \"amazonaws.com\"  |anew -q $FOLDER_NAME/S3-buckets.txt\n",
    "    print(\"Done. The file ./{}/S3-buckets.txt is updated!\".format(FOLDER_NAME)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1e1df-4106-4e8f-9e66-560b88f97830",
   "metadata": {},
   "source": [
    "### Potentially Vulnerable Endpoints\n",
    "\n",
    "Some HTTP parameter names are more commonly associated with one functionality than the others. For example, the parameter ?url= usually contains URLs as the value and hence often falls victim to file inclusion, open redirect and SSRF attacks. The following cell will create multiple files with URLs which are potentially vulnerable to lfi, ssrf, sqli, xss, open_redirect or rce. \n",
    "\n",
    "Parth: https://github.com/s0md3v/parth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea207e4-a9e3-4c25-97a5-d7c644d9d6ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Running parth ... \")\n",
    "!cp $FOLDER_NAME/Dedupe_Subdomains_urls.txt urls.txt\n",
    "    \n",
    "!cat urls.txt |parth --pipe xss |httpx -silent |anew -q $FOLDER_NAME/Potentially_XSS.txt\n",
    "!cat urls.txt |parth --pipe ssrf |httpx -silent |anew -q $FOLDER_NAME/Potentially_SSRF.txt\n",
    "!cat urls.txt |parth --pipe sqli |httpx -silent  |anew -q $FOLDER_NAME/Potentially_SQLi.txt\n",
    "!cat urls.txt |parth --pipe lfi |httpx -silent |anew -q $FOLDER_NAME/Potentially_LFI.txt\n",
    "!cat urls.txt |parth --pipe open_redirect |httpx -silent |anew -q $FOLDER_NAME/Potentially_OPEN-REDIRECT.txt\n",
    "!cat urls.txt |parth --pipe rce |httpx -silent |anew -q $FOLDER_NAME/Potentially_RCE.txt\n",
    "!rm urls.txt\n",
    "print(\"Done. The potentially vulnerable endpoints are updated to files Potentially_XSS and so on!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e02983-3c13-468b-8ba0-c73189d466ab",
   "metadata": {},
   "source": [
    "### XSS Scanner\n",
    "\n",
    "Scanning URLs found in the previous step. It checks for DOM based as well as reflected XSS.\n",
    "\n",
    "GXSS: https://github.com/KathanP19/Gxss\n",
    "\n",
    "XSStrike: https://github.com/s0md3v/XSStrike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec1e341-c5b1-42b3-80ec-5694d6d5f8e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SCAN_TYPE == \"A\" and WEB_SCAN == \"Y\":\n",
    "    print(\"Scannig Potentially_XSS.txt for XSS ...\")\n",
    "    !cp $FOLDER_NAME/Potentially_XSS.txt XSS.txt\n",
    "    !cat XSS.txt |Gxss -c 100 -o XSS_Scanner_Results_Gxss.txt >/dev/null 2>&1\n",
    "    !mv XSS_Scanner_Results_Gxss.txt $FOLDER_NAME/XSS_Scanner_Results_Gxss.txt && rm XSS.txt\n",
    "    print(\"Done. The file ./{}/XSS_Scanner_Results_Gxss.txt is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf0e33-592a-4c2c-8e5c-169a26d2b0a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "RUN_XSStrike = \"Y\" #this is for further deep scanning, specify Y/N\n",
    "if SCAN_TYPE == \"A\" and WEB_SCAN == \"Y\" and RUN_XSStrike == \"Y\":\n",
    "    print(\"Scannig Potentially_XSS.txt for XSS ...\")\n",
    "    !cp $FOLDER_NAME/Potentially_XSS.txt XSS.txt\n",
    "    !while read -r line; do echo $line >> XSS_Scanner_Results_XSStrike.txt; python3 /home/${NB_USER}/XSStrike/xsstrike.py -u $line >> XSS_Scanner_Results_XSStrike.txt; done < XSS.txt\n",
    "    !mv XSS_Scanner_Results_XSStrike.txt $FOLDER_NAME/XSS_Scanner_Results_XSStrike.txt && rm XSS.txt\n",
    "    print(\"Done. The file ./{}/XSS_Scanner_Results_XSStrike.txt is updated!\".format(FOLDER_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21ce867-dae9-4d47-b616-70743e28f9bd",
   "metadata": {},
   "source": [
    "### SSRF Scanner using HTTPX\n",
    "\n",
    "Provide your callback URL, e.g. burpsuite collaborator, in the Setup cell. The followign cell will create a text file with payloads and make HTTP requests. If you receive a callback, check the random number in the received callback and find it in the text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c915864b-086d-49e1-b035-ad998bf271fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$SSRF_URL\" \"$FOLDER_NAME\" \"$SCAN_TYPE\" \"$WEB_SCAN\"\n",
    "if [ \"$1\" != \"\" ] && [ \"$3\" == \"A\" ] && [ \"$4\" == \"Y\" ]; then \n",
    "    echo \"Adding the provided URL to the Potentially_SSRF.txt file...\"\n",
    "    cp $2/Potentially_SSRF.txt Potentially_SSRF.txt\n",
    "    sed -e \"s^=^=$1/${RANDOM}\\&zoo=^g\" Potentially_SSRF.txt|anew -q Potentially_SSRF_HTTPX.txt\n",
    "    rm Potentially_SSRF.txt\n",
    "\n",
    "    echo \"Sending HTTP Requests...\"\n",
    "    cat Potentially_SSRF_HTTPX.txt |httpx -silent >> httpx.csv\n",
    "    mv Potentially_SSRF_HTTPX.txt $2/Potentially_SSRF_HTTPX.txt\n",
    "    rm httpx.csv\n",
    "    echo \"Done. If you have received a callback, check ./$2/Potentially_SSRF_HTTPX.txt file for tracing back the URL.\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437f892f-c720-4cb4-8e15-3c27ea3b1c6d",
   "metadata": {},
   "source": [
    "### Web Fuzzing\n",
    "\n",
    "Customize the following cell as per your needs. ffuf is an amazing fuzzer, it can be used for directory bruteforcing, fuzzing headers, fuzzing parameters etc. Checkout it's help page on the gitub. \n",
    "\n",
    "The following cell does a quick directory bruteforce on all probed domains (in-scope if applicable) using small.txt from seclists (https://github.com/danielmiessler/SecLists/blob/master/Discovery/Web-Content/directory-list-2.3-small.txt).\n",
    "\n",
    "SecLists are great, do also checkout assetnote, especially for technology specific wordlists: https://wordlists.assetnote.io/\n",
    "\n",
    "\n",
    "ffuf: https://github.com/ffuf/ffuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf45b9b-d721-440d-92dc-f008504e7543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SCAN_TYPE == \"A\" and WEB_SCAN == \"Y\":\n",
    "    print(\"Running ffuf on subdomains. ... \")\n",
    "    !cp $FOLDER_NAME/Probed_Subdomains.csv subs.csv\n",
    "    !ffuf -s -u URLS/WORDS -w subs.csv:URLS -w ../small.txt:WORDS -ac >> $FOLDER_NAME/Subdomains_ffuf.txt\n",
    "    print(\"Done. The file ./{}/Subdomains_ffuf.txt is updated!\".format(FOLDER_NAME))\n",
    "    !rm subs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f24f0-7f54-4a8f-a9a2-1a912b4efa13",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f09ed2-8c1b-4e6c-9df6-6196b0f4c64b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash -s \"$FOLDER_NAME\"\n",
    "zip -r $1.zip $1 >/dev/null 2>&1\n",
    "echo \"Done. Find the zip folder in ~/work directory!\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
